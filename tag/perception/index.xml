<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Perception | Fan Li</title>
    <link>https://van23li.github.io/tag/perception/</link>
      <atom:link href="https://van23li.github.io/tag/perception/index.xml" rel="self" type="application/rss+xml" />
    <description>Perception</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 16 Jul 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://van23li.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Perception</title>
      <link>https://van23li.github.io/tag/perception/</link>
    </image>
    
    <item>
      <title>Road-side Foreground segmentation</title>
      <link>https://van23li.github.io/project/road-side-foreground-segmentation/</link>
      <pubDate>Sat, 16 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://van23li.github.io/project/road-side-foreground-segmentation/</guid>
      <description>&lt;p&gt;During my internship at BOSCH, I worked on the road-side foreground segmentation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Violence Detection</title>
      <link>https://van23li.github.io/project/violence-detection/</link>
      <pubDate>Thu, 13 May 2021 00:00:00 +0000</pubDate>
      <guid>https://van23li.github.io/project/violence-detection/</guid>
      <description>&lt;p&gt;Project for &lt;strong&gt;上海市级大学生创新创业项目&lt;/strong&gt; | &lt;strong&gt;Shanghai innovation and entrepreneurship training program&lt;/strong&gt;, &lt;em&gt;Mar. 2019 - Mar. 2020&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As shown in the picture, our project &amp;ldquo;Violence detection system based on deep learning&amp;rdquo; (Fan Li, Zhuofan Li, and Xiaoxiao Yang) supposed by 
&lt;a href=&#34;https://mefaculty.tongji.edu.cn/index.php?classid=2761&amp;amp;t=show&amp;amp;id=46&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Aiguo Zhou&lt;/a&gt; and 
&lt;a href=&#34;https://vision4robotics.github.io/authors/changhong-fu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Changhong Fu&lt;/a&gt; focuses on identifying violent individuals and tracking them. It mainly contains the following contents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reading, displaying and preprocessing videos using 
&lt;a href=&#34;https://opencv.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Using 
&lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLO&lt;/a&gt; which is a famous detection algorithm to identify the person&amp;rsquo;s position&lt;/li&gt;
&lt;li&gt;Extracting person&amp;rsquo;s joint coordinates based on 
&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenPose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Converting the joint coordinates into the angle information, and using SVM to identify violent individuals&lt;/li&gt;
&lt;li&gt;Tracking the violent individuals using the proposed tracker 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/9197252&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TSD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can find the demonstration video 
&lt;a href=&#34;https://www.bilibili.com/video/BV1x341167sB/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online Visual Object Tracking for UAV</title>
      <link>https://van23li.github.io/project/online-visual-object-tracking-for-uav/</link>
      <pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://van23li.github.io/project/online-visual-object-tracking-for-uav/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Undergraduate Research Assistant&lt;/strong&gt;, &lt;em&gt;Oct. 2018 - Oct. 2020&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;During this time, I focused on investigating visual object tracking for UAV based on correlation filter-based methods.&lt;/p&gt;
&lt;p&gt;Among discriminative visual tracking methods, correlation filter (CF)-based approaches have received considerable attention for UAV tracking due to their high computational efficiency. In those methods, a 2D discriminative correlation filter is trained by minimizing the squared error over samples and their regression targets. Benefit from element-wise operation in the frequency domain, the CF-based methods usually have fast computational speed. In detection phase, a 2D response map is first obtained by correlating the trained filter, and new samples extracted from newly came image. Then the target location is predicted according to the highest peak of the response map. A bounding box is introduced circumscribe the target, indicating its location and representing its scale.&lt;/p&gt;
&lt;p&gt;Related work has been published as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;A cooperative energy minimization function is established to score the historical samples adaptively, optimizing the training-set’s quality efficiently and effectively.

&lt;a href=&#34;https://ieeexplore.ieee.org/document/9197252&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Training-Set Distillation for Real-Time UAV Object Tracking&lt;/strong&gt;&lt;/a&gt; in ICRA 2020&lt;/li&gt;
&lt;li&gt;A dynamic constraint strategy is introduced to set up an adaptive restriction on the consistency level, exploiting rich temporal information in response maps thoroughly.

&lt;a href=&#34;https://ieeexplore.ieee.org/document/9340954&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Learning Consistency Pursued Correlation Filters for Real-Time UAV Tracking&lt;/strong&gt;&lt;/a&gt; In IROS 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Point Cloud Object Detection</title>
      <link>https://van23li.github.io/project/point-cloud-object-detection/</link>
      <pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://van23li.github.io/project/point-cloud-object-detection/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Algorithm intern in Hesai Tech&lt;/strong&gt;, &lt;em&gt;July. 2020 - Aug. 2020&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;During my internship at Hesai Tech, I focused on the point cloud-based object detection. Our work is based on 
&lt;a href=&#34;https://github.com/open-mmlab/OpenPCDet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenPCDet&lt;/a&gt; and 
&lt;a href=&#34;https://waymo.com/open/data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Waymo Open Dataset&lt;/a&gt;. Based on the fact that it is easier for human to distinguish moving point clouds, we fused information from multiple frames to raise the overall accuracy of detection.&lt;/p&gt;
&lt;p&gt;One of the challenges is to complete the evaluation toolbox. The toolbox from OpenPCDnet was developed based on 
&lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KITTI Dataset&lt;/a&gt;, although now it has been upgrated from v0.1 to v0.2 with pretty new structures to support various datasets including Waymo. Due to differences in the structures of KITTI and Waymo, we had to adjust it to support Waymo dataset. Besides, as we used multiple frames while most other methods only consider the single frame, it&amp;rsquo;s worthy to consider carefully how to evaluate them fairly.&lt;/p&gt;
&lt;p&gt;To raise the accuracy of detection using multiple frames, I developed several detectors with OpenPCDet. My favorite method is using 
&lt;a href=&#34;https://arxiv.org/abs/1506.04214&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ConvLSTM&lt;/a&gt; after bird-eye-view compression to introduce temporal information.&lt;/p&gt;
&lt;p&gt;The internship at Hesai was not only a pleasant journey, as I made many valuable friends, I also learned to solve the problem from an engineer’s perspective.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
